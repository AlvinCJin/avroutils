#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import functools
import optparse

import csv

from avro import schema, io, datafile

VERSION="0.1"

PRIMITIVE_DATA_TYPE_MAP = {
    'string': lambda x: str(x),
    'boolean': lambda x: bool(x),
    'bytes': lambda x: bytes(x),
    'int': lambda x: int(x),
    'long': lambda x: long(x),
    'float': lambda x: float(x),
    'double': lambda x: float(x),
    'null': lambda x: None,
    }

def dataTypeMapper(names, namesdict, types, defaults, lineData):
  """ Convert a given primitive data to its specified schema type. Use defaults if nulls are encountered for non sequence types."""
  # Try converting the data to its actual type.
  for index, name in enumerate(names):
    data = lineData[index]
    try:
      namesdict[name] = PRIMITIVE_DATA_TYPE_MAP[types[index]](data)
    except ValueError as valErr:
      # We failed. So lets see if we have a default value to use?
      if defaults[index]!= None:
        namesdict[name]= PRIMITIVE_DATA_TYPE_MAP[types[index]](defaults[index])
      else:
        # We tried what we could. Its definitely the data or the schema that's wrong.
        # Raise the exception and let the user ponder.
        raise valErr
  return namesdict

def createOptionParser():
  """ Create an command line options parser that supports options for various types of delimiter seperated files. """
  usage = "Usage: %prog [options] INPUTFILES... OUTPUTFILE"
  version = "%prog " + VERSION
  parser = optparse.OptionParser(usage=usage, version=version)
  parser.add_option('-d', '--delimiter', dest='delimiter', default=',', help="Specify a delimiter for the CSV file's records. Default is comma (',').")
  parser.add_option('-f', '--header', dest='header', action='store_true', default=False, help="Specify that the first record in the CSV file is (or must be) a header.")
  parser.add_option('-r', '--reverse', action='store_true', default=False, dest='reverse', help="Convertread/write as.")
  parser.add_option('-s', '--schema-file', dest='schema', help="Specify a schema to read/write as.")
  parser.add_option('-n', '--record-name', dest='name', help="Specify a record name. Used when no schema file is provided. Default is the first input filename")
  parser.add_option('-c', '--compress', dest='compress', action='store_true', default=False, help="Toggle compression of avro output file using 'deflate' codec.")
  parser.add_option('-o', '--overwrite', dest='overwrite', action='store_true', default=False, help="Overwrite existing output file if it already exists.")
  parser.add_option('-a', '--append', dest='append', action='store_true', default=False, help="Append to existing output file if it already exists.")
  return parser

def validateOptions(parser):
  """ Validate the options from the commandline and return the remaining options. """
  (opts, args) = parser.parse_args()

  # Perform any required validation
  if opts.overwrite and opts.append:
    parser.error("Can't specify both overwrite and append options together.")

  if opts.schema:
    try:
      schema.parse(open(opts.schema).read())
    except:
      parser.error("Given schema file %s is not a valid Avro schema file." % opts.schema)

  return (opts, args)

def convertCsvToAvro(parser, opts, inputs, output):
  """ Read the CSV files and convert them to Avro. """

  # Ensure that the output destination does not already exist, if no overwriting flag is set.
  if not opts.overwrite and not opts.append:
    try:
      open(output)
      parser.error("Output file already exists. Provide an --overwrite option if you want to force an overwrite or an --append option if you want to append to it.")
    except IOError:
      pass

  # Retrieve some sample data to sniff first.
  firstFile = open(inputs[0])
  sampleData = firstFile.read(5*1024) # TODO: Make this configurable as an advanced option?
  firstFile.seek(0)
  header = firstFile.readline()
  firstFile.close()

  # Unescape our delimiter, if available, for convenience.
  if opts.delimiter:
    delimiter = opts.delimiter.decode('string-escape')
  else:
    # Sniff out a dialect for the CSV file and use its delimiter.
    dialect = csv.Sniffer().sniff(sampleData)
    delimiter = dialect.delimiter

  # If we got a schema to use, lets use it.
  if opts.schema:
    try:
      outputSchema = schema.parse(open(opts.schema).read())
    except:
      parser.error("Given schema is invalid. Please provide a proper Avro schema")
  else:
    # Check if we have a header at least?
    if not csv.Sniffer().has_header(sampleData):
      parser.error("The input CSV files don't carry a header. Please provide a schema file or a proper CSV input file with headers.")

    # Get the 'record' name.
    name = inputs[0]
    if opts.name:
      name = opts.name

    # Construct record fields.
    # NOTE: All fields will default to 'string' type as headers don't tell us data types.
    #       To use your own types, provide a JSON schema instead.
    fieldList = []
    for fieldName in header.strip().split(delimiter):
      fieldList.append({'type': 'string', 'name': fieldName})

    # Schema can now be constructed.
    outputSchema = schema.RecordSchema(name, None, fieldList)

  # We have the schema ready. Lets begin converting.

  fieldNames = [field.name for field in outputSchema.fields]
  fieldNamesDict = dict((name, None) for name in fieldNames)
  fieldTypes = [field.type.type for field in outputSchema.fields]
  fieldDefaults = [field.default for field in outputSchema.fields]

  dataMapper = functools.partial(dataTypeMapper, fieldNames, fieldNamesDict, fieldTypes, fieldDefaults)

  datumWriter = io.DatumWriter(outputSchema)
  dataFileWriter = datafile.DataFileWriter(open(output, 'a+' if opts.append else 'w'), datumWriter, None if opts.append else outputSchema, 'deflate' if opts.compress else 'null')

  # Convert all given input files to avro.
  for inputfile in inputs:

    openfile = open(inputfile)

    # If CSV file has a header, ignore it by reading it out.
    if opts.header:
      openfile.readline()

    for lineno, line in enumerate(openfile):

      # Convert the data types to fit the schema.
      try:
        lineData = dataMapper(line.strip().split(delimiter))
      except ValueError as valErr:
        print("Recieved a value error on line %d of file %s." % (lineno+1, inputfile))
        raise valErr

      # Append the converted CSV record to the Avro data file.
      dataFileWriter.append(lineData)

  # We've finished converting all files. Close and exit.
  dataFileWriter.close()


def convertAvroToCsv(parser, opts, inputs, output):
  """ Read the Avro files and convert them all to CSV """

  csvFileWriter = csv.writer(open(output, 'wb'), delimiter=str(opts.delimiter.decode('string-escape')), lineterminator='\n')

  datumReader = io.DatumReader()
  if opts.schema:
    datumReader = io.DatumReader(expected=schema.parse(open(opts.schema).read()))

  # Get schema fields early on.
  if opts.schema:
    fields = schema.parse(open(opts.schema).read()).fields
  else:
    fields = datafile.DataFileReader(open(inputs[0]), datumReader).datum_reader.writers_schema.fields

  for inputfile in inputs:
    openfile = datafile.DataFileReader(open(inputfile), datumReader)

    for record in openfile:
      csvRow = [record[field.name] for field in fields]
      csvFileWriter.writerow(csvRow)

def main():
  parser = createOptionParser()

  (opts, args) = validateOptions(parser)

  if not len(args) > 1:
    parser.error('You must specify an input and an output filename.')

  inputs = args[:-1]
  output = args[-1]

  if opts.reverse:
    convertAvroToCsv(parser, opts, inputs, output)
  else:
    convertCsvToAvro(parser, opts, inputs, output)


if __name__=='__main__':
  main()
